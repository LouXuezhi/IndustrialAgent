# RAGå±‚æ¨¡å—æ–‡æ¡£

## æ¦‚è¿°
RAGï¼ˆRetrieval-Augmented Generationï¼‰å±‚æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒï¼Œè´Ÿè´£æ–‡æ¡£æ£€ç´¢ã€ä¸Šä¸‹æ–‡æ„å»ºå’Œç­”æ¡ˆç”Ÿæˆã€‚é‡‡ç”¨æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡æ£€ç´¢ + BM25ï¼‰æå‡å¬å›ç‡å’Œç›¸å…³æ€§ã€‚

## æ¨¡å—ç»“æ„

```
app/rag/
â”œâ”€â”€ pipeline.py      # RAGæµæ°´çº¿ä¸»æ§åˆ¶å™¨
â”œâ”€â”€ retriever.py     # æ··åˆæ£€ç´¢å™¨
â”œâ”€â”€ prompts.py       # æç¤ºæ¨¡æ¿ç®¡ç†
â”œâ”€â”€ ingestion.py     # æ–‡æ¡£æ‘„å–å¤„ç†
â””â”€â”€ evaluators.py    # å“åº”è¯„ä¼°
```

## 1. RAG Pipeline (`pipeline.py`)

### åŠŸèƒ½
RAG Pipelineæ˜¯RAGæµç¨‹çš„ç¼–æ’å™¨ï¼Œåè°ƒæ£€ç´¢ã€æç¤ºæ„å»ºå’Œç”Ÿæˆæ­¥éª¤ã€‚

### å…³é”®ç±»ä¸ä¾èµ–

- `RAGPipeline`ï¼šç¼–æ’æ£€ç´¢ä¸ç”Ÿæˆï¼Œä½¿ç”¨ LangChain `ChatPromptTemplate` + `ChatOpenAI`ï¼ˆOpenAI æˆ– DashScope å…¼å®¹ APIï¼‰ã€‚
- `HybridRetriever`ï¼ˆ`LangchainRetriever` ç±»å‹ï¼‰ï¼šå°è£…å‘é‡æ£€ç´¢ï¼ˆChromaï¼‰+ æ£€ç´¢æ‰©å±•ä½ã€‚
- é…ç½®æ¥è‡ª `Settings`ï¼š`llm_provider`ï¼ˆopenai/dashscopeï¼‰ã€`llm_model`ã€`openai_api_key`ã€`dashscope_*_api_key/base_url`ã€‚

### `run` æµç¨‹ï¼ˆç°çŠ¶ï¼‰
1) æ£€ç´¢ï¼š`retriever.search(query, library_ids, top_k)`ï¼ŒæŒ‰åº“è¿‡æ»¤ã€‚
2) æ„å»ºæç¤ºï¼š`ChatPromptTemplate` ç”Ÿæˆå¯¹è¯æ¨¡æ¿ï¼Œå¡«å……ä¸Šä¸‹æ–‡ä¸é—®é¢˜ã€‚
3) ç”Ÿæˆï¼šæ ¹æ® `llm_provider` é€‰æ‹©å®¢æˆ·ç«¯ï¼š
   - OpenAI: ä½¿ç”¨ `openai_api_key`ã€‚
   - DashScope: ä½¿ç”¨ `dashscope_llm_api_key` æˆ– `dashscope_api_key`ï¼Œ`base_url` å…¼å®¹ OpenAIã€‚
4) è¾“å‡ºï¼šè¿”å›ç­”æ¡ˆä¸å¼•ç”¨ã€‚

### å‘é‡åŒ–ä¸æ£€ç´¢æ³¨æ„ç‚¹
- æ–‡æ¡£ä¸Šä¼ ä»…å…¥åº“æ–‡ä»¶ä¸å…ƒæ•°æ®ï¼Œå‘é‡åŒ–éœ€è°ƒç”¨ `/api/v1/docs/documents/{id}/vectorize` åæ‰å¯æ£€ç´¢ã€‚
- `document.meta["vectorized"]` åæ˜ æ˜¯å¦å·²å†™å…¥å‘é‡åº“ï¼›å¤±è´¥åˆ™ä¸º False å¹¶è¿”å›é”™è¯¯ä¿¡æ¯ã€‚
- æ£€ç´¢æ—¶ä¾èµ–åº“ ID è¿‡æ»¤ï¼Œæ”¯æŒç§åº“/ç¾¤åº“éš”ç¦»ã€‚

## 2. æ··åˆæ£€ç´¢å™¨ (`retriever.py`)

### åŠŸèƒ½
HybridRetrieverå®ç°æ··åˆæ£€ç´¢ç­–ç•¥ï¼Œç»“åˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢çš„ä¼˜åŠ¿ã€‚

### ç±»å®šä¹‰

```python
class HybridRetriever:
    def __init__(self, vector_uri: str, embedding_model: str):
        self.vector_uri = vector_uri          # å‘é‡æ•°æ®åº“URI
        self.embedding_model = embedding_model # åµŒå…¥æ¨¡å‹åç§°
```

### æ–¹æ³•

#### `async def search(query: str, library_ids: list[uuid.UUID], top_k: int = 5) -> list[RetrievedChunk]`
**æè¿°**: æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆæŒ‰æ–‡æ¡£åº“è¿‡æ»¤ï¼‰

**å‚æ•°**:
- `query`: æŸ¥è¯¢å­—ç¬¦ä¸²
- `library_ids`: æ–‡æ¡£åº“IDåˆ—è¡¨ï¼ˆé™åˆ¶æ£€ç´¢èŒƒå›´ï¼‰
- `top_k`: è¿”å›ç»“æœæ•°é‡

**è¿”å›**: `RetrievedChunk` åˆ—è¡¨

**RetrievedChunkç»“æ„**:
```python
@dataclass
class RetrievedChunk:
    document_id: str          # æ–‡æ¡£ID
    library_id: str           # æ–‡æ¡£åº“IDï¼ˆæ–°å¢ï¼‰
    text: str                 # æ–‡æ¡£å—æ–‡æœ¬
    score: float              # ç›¸å…³æ€§åˆ†æ•°
    metadata: dict[str, Any]  # å…ƒæ•°æ®ï¼ˆæ¥æºã€é¡µç ç­‰ï¼‰
```

**é‡è¦å˜æ›´**: 
- æ£€ç´¢ç»“æœå¿…é¡»åŒ…å«`library_id`å­—æ®µ
- æ£€ç´¢æ—¶æŒ‰`library_ids`è¿‡æ»¤ï¼Œåªè¿”å›æŒ‡å®šæ–‡æ¡£åº“ä¸­çš„æ–‡æ¡£å—

### å½“å‰å®ç°çŠ¶æ€
- âš ï¸ å½“å‰ä¸ºå ä½å®ç°ï¼Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
- ğŸ”„ éœ€è¦å®ç°çœŸå®çš„å‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢

### å®ç°æ–¹æ¡ˆ

#### æ–¹æ¡ˆ1: å‘é‡æ£€ç´¢ + BM25èåˆ
```python
async def search(self, query: str, top_k: int = 5) -> list[RetrievedChunk]:
    # 1. å‘é‡æ£€ç´¢
    query_embedding = await self._embed_query(query)
    vector_results = await self._vector_search(query_embedding, top_k * 2)
    
    # 2. BM25æ£€ç´¢
    bm25_results = await self._bm25_search(query, top_k * 2)
    
    # 3. ç»“æœèåˆï¼ˆReciprocal Rank Fusionï¼‰
    fused_results = self._rrf_fusion(vector_results, bm25_results)
    
    # 4. é‡æ’åºï¼ˆå¯é€‰ï¼‰
    reranked = await self._rerank(query, fused_results[:top_k * 2])
    
    return reranked[:top_k]
```

#### æ–¹æ¡ˆ2: å‘é‡æ•°æ®åº“é›†æˆï¼ˆChromaç¤ºä¾‹ï¼‰
```python
import chromadb
from chromadb.config import Settings as ChromaSettings

class HybridRetriever:
    def __init__(self, vector_uri: str, embedding_model: str):
        self.client = chromadb.PersistentClient(path=vector_uri)
        self.collection = self.client.get_or_create_collection("documents")
        self.embedding_model = embedding_model
    
    async def _embed_query(self, query: str) -> list[float]:
        # è°ƒç”¨åµŒå…¥æ¨¡å‹API
        ...
    
    async def _vector_search(self, embedding: list[float], top_k: int):
        results = self.collection.query(
            query_embeddings=[embedding],
            n_results=top_k
        )
        return results
```

#### æ–¹æ¡ˆ3: BM25å®ç°
```python
from rank_bm25 import BM25Okapi

class HybridRetriever:
    def __init__(self, ...):
        ...
        self.bm25_index = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    async def _build_bm25_index(self):
        # ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰æ–‡æ¡£å—
        chunks = await self._load_all_chunks()
        tokenized_corpus = [chunk.split() for chunk in chunks]
        self.bm25_index = BM25Okapi(tokenized_corpus)
    
    async def _bm25_search(self, query: str, top_k: int):
        if self.bm25_index is None:
            await self._build_bm25_index()
        
        tokenized_query = query.split()
        scores = self.bm25_index.get_scores(tokenized_query)
        top_indices = np.argsort(scores)[-top_k:][::-1]
        return [chunks[i] for i in top_indices]
```

### æ£€ç´¢ç­–ç•¥å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| å‘é‡æ£€ç´¢ | è¯­ä¹‰ç†è§£å¼ºï¼Œæ”¯æŒå¤šè¯­è¨€ | éœ€è¦åµŒå…¥æ¨¡å‹ï¼Œè®¡ç®—æˆæœ¬é«˜ | è¯­ä¹‰ç›¸ä¼¼æŸ¥è¯¢ |
| BM25 | å…³é”®è¯åŒ¹é…ç²¾ç¡®ï¼Œé€Ÿåº¦å¿« | æ— æ³•ç†è§£è¯­ä¹‰ | ç²¾ç¡®å…³é”®è¯æŸ¥è¯¢ |
| æ··åˆæ£€ç´¢ | å…¼é¡¾è¯­ä¹‰å’Œå…³é”®è¯ | å®ç°å¤æ‚ï¼Œéœ€è¦èåˆç­–ç•¥ | ç”Ÿäº§ç¯å¢ƒæ¨è |

## 3. æç¤ºæ¨¡æ¿ (`prompts.py`)

### åŠŸèƒ½
ç®¡ç†ä¸åŒè§’è‰²å’Œåœºæ™¯çš„æç¤ºæ¨¡æ¿ï¼Œæ”¯æŒä¸ªæ€§åŒ–ç­”æ¡ˆç”Ÿæˆã€‚

### æ¨¡æ¿å®šä¹‰

#### åŸºç¡€æ¨¡æ¿
```python
BASE_PROMPT = """You are an expert {role} for an industrial company.
Answer the user's question using ONLY the provided context.

Context:
{context}

Question: {question}
Helpful answer:"""
```

#### è§’è‰²ç‰¹å®šæ¨¡æ¿
- **operator**: è¿ç»´æŠ€æœ¯äººå‘˜è§†è§’ - æŠ€æœ¯ç»†èŠ‚ã€æ“ä½œæ­¥éª¤ã€æ•…éšœæ’æŸ¥
- **maintenance**: ç»´æŠ¤å·¥ç¨‹å¸ˆè§†è§’ - ç»´æŠ¤æµç¨‹ã€æ•…éšœæ’æŸ¥ã€é¢„é˜²æ€§ç»´æŠ¤
- **manager**: å·¥å‚ç®¡ç†è€…è§†è§’ - å†³ç­–æ”¯æŒã€æ•°æ®åˆ†æã€æˆ˜ç•¥è§„åˆ’
- **admin**: ç³»ç»Ÿç®¡ç†å‘˜è§†è§’ - ç³»ç»Ÿç®¡ç†ã€é…ç½®ä¼˜åŒ–

### å‡½æ•°

#### `def get_prompt(role: str | None = None) -> str`
**æè¿°**: æ ¹æ®è§’è‰²è·å–æç¤ºæ¨¡æ¿

**å‚æ•°**:
- `role`: ç”¨æˆ·è§’è‰²ï¼ˆoperator/maintenance/manager/adminï¼‰

**è¿”å›**: æ ¼å¼åŒ–çš„æç¤ºå­—ç¬¦ä¸²

**è§’è‰²ä¸Promptçš„å…³è”**:
è§’è‰²ä¸ä»…å½±å“æƒé™ï¼Œè¿˜ç›´æ¥å½±å“Agentçš„å›ç­”é£æ ¼å’Œå†…å®¹é‡ç‚¹ï¼š

1. **operatorè§’è‰²**:
   - Promptå¼ºè°ƒæ“ä½œæ­¥éª¤å’ŒæŠ€æœ¯ç»†èŠ‚
   - é€‚åˆå›ç­”"å¦‚ä½•æ“ä½œ"ã€"æ­¥éª¤æ˜¯ä»€ä¹ˆ"ç­‰é—®é¢˜
   - ç¤ºä¾‹: "ä½œä¸ºè¿ç»´æŠ€æœ¯äººå‘˜ï¼Œè¯·æä¾›è¯¦ç»†çš„æ“ä½œæ­¥éª¤..."

2. **maintenanceè§’è‰²**:
   - Promptå¼ºè°ƒç»´æŠ¤æµç¨‹å’Œæ•…éšœæ’æŸ¥
   - é€‚åˆå›ç­”"å¦‚ä½•ç»´æŠ¤"ã€"æ•…éšœå¦‚ä½•æ’æŸ¥"ç­‰é—®é¢˜
   - ç¤ºä¾‹: "ä½œä¸ºç»´æŠ¤å·¥ç¨‹å¸ˆï¼Œè¯·æä¾›ç»´æŠ¤æµç¨‹å’Œæ•…éšœæ’æŸ¥æ–¹æ³•..."

3. **managerè§’è‰²**:
   - Promptå¼ºè°ƒå†³ç­–æ”¯æŒå’Œæ•°æ®åˆ†æ
   - é€‚åˆå›ç­”"å¦‚ä½•å†³ç­–"ã€"æ•°æ®åˆ†æ"ç­‰é—®é¢˜
   - ç¤ºä¾‹: "ä½œä¸ºå·¥å‚ç®¡ç†è€…ï¼Œè¯·æä¾›å†³ç­–å»ºè®®å’Œæ•°æ®åˆ†æ..."

4. **adminè§’è‰²**:
   - Promptå¼ºè°ƒç³»ç»Ÿç®¡ç†å’Œé…ç½®
   - é€‚åˆå›ç­”ç³»ç»Ÿé…ç½®å’Œç®¡ç†ç›¸å…³é—®é¢˜

### æ¨¡æ¿æ‰©å±•

æ·»åŠ æ–°è§’è‰²æ¨¡æ¿ï¼š
```python
ROLE_PROMPTS["safety_officer"] = BASE_PROMPT.format(
    role="safety officer",
    context="{context}",
    question="{question}"
)
```

æ·»åŠ åœºæ™¯ç‰¹å®šæ¨¡æ¿ï¼š
```python
SCENARIO_PROMPTS = {
    "troubleshooting": """You are troubleshooting an industrial issue.
    Analyze the context and provide step-by-step solutions.
    
    Context: {context}
    Problem: {question}
    Solution:""",
    
    "maintenance_schedule": """Generate a maintenance schedule based on the provided documentation.
    ...
    """
}
```

## 4. æ–‡æ¡£æ‘„å– (`ingestion.py`)

### åŠŸèƒ½
å¤„ç†æ–‡æ¡£ä¸Šä¼ ï¼Œè¿›è¡Œåˆ†å—ã€å‘é‡åŒ–å’Œå­˜å‚¨ã€‚

### ç±»å®šä¹‰

```python
class DocumentIngestor:
    async def ingest_file(self, path: Path, session: AsyncSession) -> IngestionReport
```

### æ–¹æ³•

#### `async def ingest_file(path: Path, session: AsyncSession) -> IngestionReport`
**æè¿°**: æ‘„å–å•ä¸ªæ–‡æ¡£æ–‡ä»¶

**å‚æ•°**:
- `path`: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
- `session`: æ•°æ®åº“ä¼šè¯

**è¿”å›** (`IngestionReport`):
```python
@dataclass
class IngestionReport:
    document_id: uuid.UUID    # æ–‡æ¡£UUID
    chunk_count: int          # ç”Ÿæˆçš„å—æ•°é‡
```

### å½“å‰å®ç°çŠ¶æ€
- âš ï¸ å½“å‰ä¸ºç®€åŒ–å ä½å®ç°
- ğŸ”„ éœ€è¦å®ç°å®Œæ•´çš„åˆ†å—ã€å‘é‡åŒ–å’Œå­˜å‚¨æµç¨‹

### å®Œæ•´å®ç°æ–¹æ¡ˆ

```python
class DocumentIngestor:
    def __init__(self, retriever: HybridRetriever):
        self.retriever = retriever
    
    async def ingest_file(self, path: Path, session: AsyncSession) -> IngestionReport:
        # 1. è§£ææ–‡æ¡£
        text = await self._parse_document(path)
        
        # 2. åˆ†å—
        chunks = self._chunk_text(text, chunk_size=500, overlap=50)
        
        # 3. åˆ›å»ºæ–‡æ¡£è®°å½•
        document = await crud.create_document(
            session,
            title=path.name,
            source_path=str(path),
            metadata={"file_type": path.suffix}
        )
        
        # 4. å‘é‡åŒ–å’Œå­˜å‚¨
        chunk_count = 0
        for i, chunk_text in enumerate(chunks):
            # ç”ŸæˆåµŒå…¥
            embedding = await self._embed_text(chunk_text)
            
            # å­˜å‚¨åˆ°å‘é‡åº“
            await self.retriever._store_chunk(
                document_id=str(document.id),
                chunk_id=f"{document.id}_{i}",
                text=chunk_text,
                embedding=embedding,
                metadata={"chunk_index": i}
            )
            
            # å­˜å‚¨å…ƒæ•°æ®åˆ°æ•°æ®åº“
            chunk = models.Chunk(
                document_id=document.id,
                content=chunk_text,
                metadata={"chunk_index": i}
            )
            session.add(chunk)
            chunk_count += 1
        
        await session.commit()
        return IngestionReport(document_id=document.id, chunk_count=chunk_count)
    
    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> list[str]:
        # ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap
        )
        return splitter.split_text(text)
```

## 5. å“åº”è¯„ä¼° (`evaluators.py`)

### åŠŸèƒ½
è¯„ä¼°ç”Ÿæˆç­”æ¡ˆçš„è´¨é‡ï¼ŒåŒ…æ‹¬å¿ å®åº¦ã€ç›¸å…³æ€§ç­‰æŒ‡æ ‡ã€‚

### å‡½æ•°

#### `async def evaluate_response(answer: str, references: list[dict]) -> EvaluationResult`
**æè¿°**: è¯„ä¼°ç­”æ¡ˆè´¨é‡

**å‚æ•°**:
- `answer`: ç”Ÿæˆçš„ç­”æ¡ˆ
- `references`: å¼•ç”¨æ¥æºåˆ—è¡¨

**è¿”å›** (`EvaluationResult`):
```python
@dataclass
class EvaluationResult:
    faithfulness: float    # å¿ å®åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
    relevance: float       # ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
    notes: str            # è¯„ä¼°å¤‡æ³¨
```

### å½“å‰å®ç°çŠ¶æ€
- âš ï¸ å½“å‰ä¸ºå ä½å®ç°
- ğŸ”„ éœ€è¦é›†æˆè¯„ä¼°æ¨¡å‹ï¼ˆå¦‚GPT-4ä½œä¸ºJudgeï¼‰

### è¯„ä¼°å®ç°æ–¹æ¡ˆ

#### æ–¹æ¡ˆ1: LLMä½œä¸ºJudge
```python
async def evaluate_response(answer: str, references: list[dict]) -> EvaluationResult:
    judge_prompt = f"""Evaluate the following answer:
    
    Answer: {answer}
    References: {references}
    
    Rate on a scale of 0-1:
    1. Faithfulness: Does the answer stay true to the references?
    2. Relevance: Does the answer address the question?
    
    Respond in JSON format:
    {{"faithfulness": 0.9, "relevance": 0.8, "notes": "..."}}
    """
    
    response = await llm_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": judge_prompt}],
        response_format={"type": "json_object"}
    )
    
    result = json.loads(response.choices[0].message.content)
    return EvaluationResult(**result)
```

#### æ–¹æ¡ˆ2: åŸºäºè§„åˆ™çš„è¯„ä¼°
```python
def evaluate_response(answer: str, references: list[dict]) -> EvaluationResult:
    # å¿ å®åº¦ï¼šæ£€æŸ¥ç­”æ¡ˆä¸­çš„å…³é”®ä¿¡æ¯æ˜¯å¦åœ¨å¼•ç”¨ä¸­
    faithfulness = check_faithfulness(answer, references)
    
    # ç›¸å…³æ€§ï¼šæ£€æŸ¥ç­”æ¡ˆé•¿åº¦ã€å¼•ç”¨æ•°é‡ç­‰
    relevance = check_relevance(answer, references)
    
    return EvaluationResult(
        faithfulness=faithfulness,
        relevance=relevance,
        notes="Rule-based evaluation"
    )
```

## RAGæµç¨‹ä¼˜åŒ–å»ºè®®

1. **æ£€ç´¢ä¼˜åŒ–**:
   - å®ç°æŸ¥è¯¢æ‰©å±•ï¼ˆQuery Expansionï¼‰
   - æ·»åŠ é‡æ’åºï¼ˆRerankingï¼‰æ­¥éª¤
   - æ”¯æŒå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡

2. **ç”Ÿæˆä¼˜åŒ–**:
   - å®ç°æµå¼ç”Ÿæˆï¼ˆStreamingï¼‰
   - æ”¯æŒå¼•ç”¨æ ‡æ³¨ï¼ˆCitationï¼‰
   - æ·»åŠ ç­”æ¡ˆç½®ä¿¡åº¦è¯„åˆ†

3. **è¯„ä¼°ä¼˜åŒ–**:
   - å®ç°A/Bæµ‹è¯•æ¡†æ¶
   - æ”¶é›†ç”¨æˆ·åé¦ˆå¹¶é—­ç¯ä¼˜åŒ–
   - å»ºç«‹è¯„ä¼°æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•

