from dataclasses import dataclass
from typing import Any
from uuid import UUID
import asyncio
import logging

from langchain_community.vectorstores import Chroma
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from chromadb.utils import embedding_functions
import chromadb
import numpy as np

from app.core.config import get_settings, Settings
from app.rag.ingestion import _resolve_chroma_path
from app.rag.reranker import Reranker
from app.rag.synonyms import QueryExpander, SynonymDict

logger = logging.getLogger(__name__)


@dataclass
class RetrievedChunk:
    document_id: str
    text: str
    score: float
    metadata: dict[str, Any]
    source_type: str = "vector"  # "vector", "bm25", or "hybrid"


def _build_embedding_fn(settings: Settings):
    # Prefer configured embedding provider; fallback to default.
    if settings.llm_provider == "dashscope" and (
        settings.dashscope_embedding_api_key or settings.dashscope_api_key
    ):
        # DashScope provides an OpenAI-compatible endpoint; use api_base to direct traffic.
        return embedding_functions.OpenAIEmbeddingFunction(
            api_key=settings.dashscope_embedding_api_key or settings.dashscope_api_key,
            model_name=settings.embedding_model,
            api_base=settings.dashscope_embedding_base_url or settings.dashscope_base_url,
        )
    if settings.llm_provider == "openai" and settings.openai_api_key:
        return embedding_functions.OpenAIEmbeddingFunction(
            api_key=settings.openai_api_key,
            model_name=settings.embedding_model,
        )
    return embedding_functions.DefaultEmbeddingFunction()


class LangchainRetriever:
    """LangChain-based Chroma retriever with library scoping."""

    def __init__(self, vector_uri: str, settings: Settings | None = None) -> None:
        self.settings = settings or get_settings()
        self.chroma_path = _resolve_chroma_path(vector_uri)
        self.embedding_fn = _build_embedding_fn(self.settings)

    def _collection_name(self, library_id: UUID | None) -> str:
        return f"library_{library_id}" if library_id else "library_default"

    def _get_vectorstore(self, library_id: UUID | None) -> Chroma:
        return Chroma(
            collection_name=self._collection_name(library_id),
            embedding_function=self.embedding_fn,
            persist_directory=self.chroma_path,
        )
    
    def _get_chroma_collection(self, library_id: UUID | None):
        """Get ChromaDB collection directly (bypassing LangChain).
        
        Uses the collection's existing embedding function if available,
        otherwise uses the configured embedding function.
        
        Returns None if collection doesn't exist and cannot be created.
        """
        import chromadb.errors
        client = chromadb.PersistentClient(path=self.chroma_path)
        collection_name = self._collection_name(library_id)
        try:
            # Try to get existing collection (will use its original embedding function)
            collection = client.get_collection(name=collection_name)
            # If collection exists, it will use its original embedding function
            # We need to ensure we use the same one for queries
            return collection
        except chromadb.errors.NotFoundError:
            # Collection doesn't exist - return None instead of creating
            # Collections should be created during document vectorization
            logger.debug(f"Collection {collection_name} does not exist (library_id: {library_id})")
            return None
        except Exception as e:
            logger.error(f"Error getting ChromaDB collection {collection_name}: {e}", exc_info=True)
            return None

    async def search(
        self,
        query: str,
        top_k: int = 5,
        library_ids: list[UUID] | None = None
    ) -> list[RetrievedChunk]:
        library_ids_to_search = library_ids if library_ids else [None]
        results: list[RetrievedChunk] = []

        async def _search_collection(lib_id: UUID | None):
            try:
                import logging
                logger = logging.getLogger(__name__)
                logger.debug(f"Searching collection for library_id: {lib_id}")
                
                # Use ChromaDB directly to avoid LangChain embedding format issues
                collection = self._get_chroma_collection(lib_id)
                
                # Query using ChromaDB's native API
                query_results = await asyncio.to_thread(
                    collection.query,
                    query_texts=[query],
                    n_results=top_k
                )
                
                if not query_results or not query_results.get('ids') or not query_results['ids'][0]:
                    logger.warning(f"No documents found in collection for library_id: {lib_id}")
                    return
                
                # Extract results
                ids = query_results['ids'][0]
                documents = query_results['documents'][0]
                metadatas = query_results['metadatas'][0] if query_results.get('metadatas') else [{}] * len(ids)
                distances = query_results['distances'][0] if query_results.get('distances') else [0.0] * len(ids)
                
                logger.debug(f"Found {len(ids)} documents in collection for library_id: {lib_id}")
                
                for i, (doc_id, doc_text, meta, distance) in enumerate(zip(ids, documents, metadatas, distances)):
                    # Convert distance to similarity (lower distance = higher similarity)
                    similarity = 1.0 / (1.0 + abs(distance)) if distance != 0 else 1.0
                    
                    results.append(
            RetrievedChunk(
                            document_id=str(meta.get("document_id", doc_id)),
                            text=doc_text,
                            score=similarity,
                            metadata=meta,
                        )
                    )
            except Exception as e:
                # Log error but continue to allow partial results from other stores
                import logging
                logger = logging.getLogger(__name__)
                logger.error(f"Error searching vector store for library_id {lib_id}: {e}", exc_info=True)
                return

        await asyncio.gather(*[_search_collection(lib_id) for lib_id in library_ids_to_search])
        
        if not results:
            import logging
            logger = logging.getLogger(__name__)
            logger.warning(f"No results found for query: {query}, library_ids: {library_ids}")
            return []
        
        # Sort by similarity (descending)
        results.sort(key=lambda c: c.score, reverse=True)
        return results[:top_k]


def _weighted_reciprocal_rank(
    vector_results: list[RetrievedChunk],
    bm25_results: list[RetrievedChunk],
    k: int = 60
) -> list[RetrievedChunk]:
    """
    RRF (å€’æ•°æ’åèåˆ) ç®—æ³•æ ¸å¿ƒ
    å…¬å¼: Score = 1 / (k + rank)
    
    Args:
        vector_results: å‘é‡æ£€ç´¢ç»“æœ
        bm25_results: BM25 æ£€ç´¢ç»“æœ
        k: RRF å¸¸æ•°ï¼Œé»˜è®¤ 60ï¼ˆå¸¸è§å€¼ï¼‰
    
    Returns:
        èåˆåçš„æ£€ç´¢ç»“æœï¼ŒæŒ‰ RRF åˆ†æ•°é™åºæ’åˆ—
    """
    fused_scores: dict[str, float] = {}
    chunk_map: dict[str, RetrievedChunk] = {}
    
    # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
    for rank, chunk in enumerate(vector_results):
        # ä½¿ç”¨ document_id + text çš„å‰50å­—ç¬¦ä½œä¸ºå”¯ä¸€é”®ï¼ˆé¿å…é‡å¤ï¼‰
        chunk_key = f"{chunk.document_id}:{chunk.text[:50]}"
        
        if chunk_key not in chunk_map:
            chunk_map[chunk_key] = chunk
            fused_scores[chunk_key] = 0.0
        
        # RRF åˆ†æ•°ç´¯åŠ ï¼šæ’åè¶Šé å‰(rankå°)ï¼Œåˆ†æ•°è¶Šé«˜
        fused_scores[chunk_key] += 1.0 / (k + rank + 1)
    
    # å¤„ç† BM25 æ£€ç´¢ç»“æœ
    for rank, chunk in enumerate(bm25_results):
        chunk_key = f"{chunk.document_id}:{chunk.text[:50]}"
        
        if chunk_key not in chunk_map:
            chunk_map[chunk_key] = chunk
            fused_scores[chunk_key] = 0.0
        
        # RRF åˆ†æ•°ç´¯åŠ 
        fused_scores[chunk_key] += 1.0 / (k + rank + 1)
    
    # æŒ‰ RRF åˆ†æ•°é™åºæ’åˆ—
    sorted_keys = sorted(fused_scores.keys(), key=lambda x: fused_scores[x], reverse=True)
    
    # è½¬æ¢ä¸ºæœ€ç»ˆçš„ RetrievedChunk åˆ—è¡¨
    final_chunks = []
    for chunk_key in sorted_keys:
        chunk = chunk_map[chunk_key]
        # æ›´æ–°åˆ†æ•°ä¸º RRF èåˆåˆ†æ•°ï¼Œæ ‡è®°æ¥æºä¸ºæ··åˆ
        final_chunk = RetrievedChunk(
            document_id=chunk.document_id,
            text=chunk.text,
            score=fused_scores[chunk_key],
            metadata=chunk.metadata,
            source_type="hybrid"
        )
        final_chunks.append(final_chunk)
    
    return final_chunks


class HybridRetriever(LangchainRetriever):
    """
    æ··åˆæ£€ç´¢å™¨ï¼šç»“åˆå‘é‡æ£€ç´¢ï¼ˆChromaDBï¼‰å’Œå…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰
    ä½¿ç”¨ RRFï¼ˆå€’æ•°æ’åèåˆï¼‰ç®—æ³•åˆå¹¶ç»“æœï¼Œæå‡æ£€ç´¢è´¨é‡ã€‚
    
    æ”¯æŒ BM25 ç´¢å¼•çš„å¢é‡æ›´æ–°ï¼š
    - è·Ÿè¸ªæ¯ä¸ªåº“çš„æ–‡æ¡£å˜æ›´ï¼ˆæ·»åŠ /åˆ é™¤/æ›´æ–°ï¼‰
    - å»¶è¿Ÿé‡å»ºï¼šä»…åœ¨æ£€ç´¢æ—¶æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
    - æ™ºèƒ½é‡å»ºï¼šåªé‡å»ºæœ‰å˜æ›´çš„åº“çš„ç´¢å¼•
    """
    
    def __init__(self, vector_uri: str, settings: Settings | None = None, enable_rerank: bool = True) -> None:
        super().__init__(vector_uri, settings)
        # BM25 æ£€ç´¢å™¨ç¼“å­˜ï¼š{library_id: BM25Retriever}
        self._bm25_retrievers: dict[str | None, BM25Retriever | None] = {}
        # æ–‡æ¡£å˜æ›´è·Ÿè¸ªï¼š{library_id: set(chunk_ids)} - è®°å½•å·²å˜æ›´çš„æ–‡æ¡£ID
        self._dirty_libraries: dict[str | None, set[str]] = {}
        # æ–‡æ¡£è®¡æ•°ç¼“å­˜ï¼š{library_id: count} - ç”¨äºæ£€æµ‹æ–‡æ¡£æ•°é‡å˜åŒ–
        self._document_counts: dict[str | None, int] = {}
        # é‡æ’åºå™¨
        self.reranker = Reranker(
            enable=enable_rerank,
            enable_cache=settings.rerank_cache_enable
        )
        # æŸ¥è¯¢æ‰©å±•å™¨
        self.settings = settings or get_settings()
        synonym_dict_path = self.settings.synonym_dict_path if self.settings.synonym_dict_path else None
        synonym_dict = SynonymDict(dict_path=synonym_dict_path) if synonym_dict_path else None
        enable_expansion = self.settings.enable_query_expansion if self.settings else True
        self.query_expander = QueryExpander(
            synonym_dict=synonym_dict,
            enable=enable_expansion
        )
        logger.info("ğŸ”„ åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡ + BM25 + é‡æ’åº + æŸ¥è¯¢æ‰©å±•ï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼‰")
    
    def _reload_bm25_index(self, library_id: UUID | None, force: bool = False) -> BM25Retriever | None:
        """
        ä» ChromaDB åŠ è½½æŒ‡å®šåº“çš„æ‰€æœ‰æ–‡æ¡£å¹¶æ„å»º BM25 ç´¢å¼•ã€‚
        
        Args:
            library_id: æ–‡æ¡£åº“IDï¼ŒNone è¡¨ç¤ºé»˜è®¤åº“
            force: æ˜¯å¦å¼ºåˆ¶é‡å»ºï¼ˆå¿½ç•¥å˜æ›´æ£€æµ‹ï¼‰
        
        Returns:
            BM25Retriever å®ä¾‹ï¼Œå¦‚æœåº“ä¸ºç©ºæˆ–å‡ºé”™åˆ™è¿”å› None
        """
        try:
            collection = self._get_chroma_collection(library_id)
            
            # Collection doesn't exist - cannot build BM25 index
            if collection is None:
                logger.debug(f"Collection for library {library_id} does not exist, skipping BM25 index build")
                lib_key = str(library_id) if library_id else None
                self._bm25_retrievers[lib_key] = None
                return None
            
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_docs_data = collection.get()
            
            if not all_docs_data.get('documents') or not all_docs_data['documents']:
                logger.debug(f"Library {library_id} ä¸ºç©ºï¼Œè·³è¿‡ BM25 ç´¢å¼•æ„å»º")
                return None
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºï¼ˆé€šè¿‡æ–‡æ¡£æ•°é‡å˜åŒ–ï¼‰
            lib_key = str(library_id) if library_id else None
            current_count = len(all_docs_data['documents'])
            cached_count = self._document_counts.get(lib_key, 0)
            
            # å¦‚æœæ–‡æ¡£æ•°é‡æ²¡å˜ä¸”ä¸æ˜¯å¼ºåˆ¶é‡å»ºï¼Œä¸”æ²¡æœ‰æ ‡è®°ä¸ºè„æ•°æ®ï¼Œå°è¯•å¤ç”¨ç°æœ‰ç´¢å¼•
            if not force and current_count == cached_count and lib_key not in self._dirty_libraries:
                existing_retriever = self._bm25_retrievers.get(lib_key)
                if existing_retriever is not None:
                    logger.debug(f"Library {library_id} ç´¢å¼•æœªå˜æ›´ï¼Œå¤ç”¨ç°æœ‰ç´¢å¼•")
                    return existing_retriever
            
            # éœ€è¦é‡å»ºç´¢å¼•
            logger.info(f"ğŸ”„ é‡å»º BM25 ç´¢å¼• (library_id: {library_id}), æ–‡æ¡£æ•°: {cached_count} â†’ {current_count}")
            
            # é‡ç»„ä¸º LangChain Document å¯¹è±¡åˆ—è¡¨
            docs = []
            documents = all_docs_data['documents']
            metadatas = all_docs_data.get('metadatas', [{}] * len(documents))
            ids = all_docs_data.get('ids', [])
            
            for i, text in enumerate(documents):
                meta = metadatas[i] if i < len(metadatas) else {}
                # ç¡®ä¿ id å­˜åœ¨
                doc_id = ids[i] if i < len(ids) else f"doc_{i}"
                meta['id'] = doc_id
                docs.append(Document(page_content=text, metadata=meta))
            
            # æ„å»º BM25 æ£€ç´¢å™¨
            bm25_retriever = BM25Retriever.from_documents(docs)
            # è®¾ç½®è¿”å›æ•°é‡ï¼ˆç¨å¤§äº top_kï¼Œä»¥ä¾¿åç»­èåˆæ—¶æœ‰æ›´å¤šå€™é€‰ï¼‰
            bm25_retriever.k = 20
            
            # æ›´æ–°ç¼“å­˜
            self._document_counts[lib_key] = current_count
            # æ¸…é™¤è„æ•°æ®æ ‡è®°
            if lib_key in self._dirty_libraries:
                del self._dirty_libraries[lib_key]
            
            logger.info(f"âœ… BM25 ç´¢å¼•æ„å»ºå®Œæˆ (library_id: {library_id}), å…± {len(docs)} æ¡æ–‡æ¡£")
            return bm25_retriever
            
        except Exception as e:
            logger.error(f"âŒ BM25 ç´¢å¼•æ„å»ºå¤±è´¥ (library_id: {library_id}): {e}", exc_info=True)
            return None
    
    def _get_bm25_retriever(self, library_id: UUID | None, force_rebuild: bool = False) -> BM25Retriever | None:
        """
        è·å–æŒ‡å®šåº“çš„ BM25 æ£€ç´¢å™¨ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–éœ€è¦æ›´æ–°åˆ™æ„å»ºã€‚
        
        Args:
            library_id: æ–‡æ¡£åº“ID
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
        
        Returns:
            BM25Retriever å®ä¾‹ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å› None
        """
        lib_key = str(library_id) if library_id else None
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        needs_rebuild = (
            force_rebuild or
            lib_key not in self._bm25_retrievers or
            self._bm25_retrievers[lib_key] is None or
            lib_key in self._dirty_libraries
        )
        
        if needs_rebuild:
            # é‡å»ºç´¢å¼•ï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ•°é‡å˜åŒ–ï¼‰
            bm25_retriever = self._reload_bm25_index(library_id, force=force_rebuild)
            self._bm25_retrievers[lib_key] = bm25_retriever
            return bm25_retriever
        else:
            # å¤ç”¨ç°æœ‰ç´¢å¼•
            return self._bm25_retrievers[lib_key]
    
    async def _vector_search(
        self,
        query: str,
        library_id: UUID | None,
        top_k: int
    ) -> list[RetrievedChunk]:
        """æ‰§è¡Œå‘é‡æ£€ç´¢ï¼ˆå¤ç”¨çˆ¶ç±»é€»è¾‘ï¼‰"""
        results: list[RetrievedChunk] = []
        
        try:
            collection = self._get_chroma_collection(library_id)
            
            # Collection doesn't exist - return empty results
            if collection is None:
                logger.debug(f"Collection for library {library_id} does not exist, returning empty results")
                return results
            
            query_results = await asyncio.to_thread(
                collection.query,
                query_texts=[query],
                n_results=top_k
            )
            
            if not query_results or not query_results.get('ids') or not query_results['ids'][0]:
                return results
            
            ids = query_results['ids'][0]
            documents = query_results['documents'][0]
            metadatas = query_results['metadatas'][0] if query_results.get('metadatas') else [{}] * len(ids)
            distances = query_results['distances'][0] if query_results.get('distances') else [0.0] * len(ids)
            
            for doc_id, doc_text, meta, distance in zip(ids, documents, metadatas, distances):
                similarity = 1.0 / (1.0 + abs(distance)) if distance != 0 else 1.0
                results.append(
                    RetrievedChunk(
                        document_id=str(meta.get("document_id", doc_id)),
                        text=doc_text,
                        score=similarity,
                        metadata=meta,
                        source_type="vector"
                    )
                )
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥ (library_id: {library_id}): {e}", exc_info=True)
        
        return results
    
    async def _bm25_search(
        self,
        query: str,
        library_id: UUID | None,
        top_k: int
    ) -> list[RetrievedChunk]:
        """æ‰§è¡Œ BM25 å…³é”®è¯æ£€ç´¢"""
        results: list[RetrievedChunk] = []
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»ºï¼ˆå¦‚æœæœ‰è„æ•°æ®æ ‡è®°ï¼‰
            lib_key = str(library_id) if library_id else None
            force_rebuild = lib_key in self._dirty_libraries
            
            bm25_retriever = self._get_bm25_retriever(library_id, force_rebuild=force_rebuild)
            if not bm25_retriever:
                return results
            
            # æ‰§è¡Œ BM25 æ£€ç´¢ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œä½¿ç”¨ asyncio.to_threadï¼‰
            bm25_docs = await asyncio.to_thread(bm25_retriever.invoke, query)
            
            # è½¬æ¢ä¸º RetrievedChunk
            for doc in bm25_docs[:top_k]:
                results.append(
                    RetrievedChunk(
                        document_id=str(doc.metadata.get("document_id", doc.metadata.get("id", "unknown"))),
                        text=doc.page_content,
                        score=1.0,  # BM25 åˆ†æ•°åœ¨ RRF ä¸­é€šè¿‡æ’åä½“ç°
                        metadata=doc.metadata,
                        source_type="bm25"
                    )
                )
        except Exception as e:
            logger.error(f"BM25 æ£€ç´¢å¤±è´¥ (library_id: {library_id}): {e}", exc_info=True)
        
        return results
    
    async def search(
        self,
        query: str,
        top_k: int = 5,
        library_ids: list[UUID] | None = None,
        use_hybrid: bool = True
    ) -> list[RetrievedChunk]:
        """
        æ‰§è¡Œæ··åˆæ£€ç´¢ï¼ˆå‘é‡ + BM25ï¼‰æˆ–çº¯å‘é‡æ£€ç´¢ã€‚
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            library_ids: è¦æœç´¢çš„æ–‡æ¡£åº“IDåˆ—è¡¨ï¼ŒNone è¡¨ç¤ºæœç´¢æ‰€æœ‰åº“
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆé»˜è®¤ Trueï¼‰ï¼ŒFalse åˆ™ä»…ä½¿ç”¨å‘é‡æ£€ç´¢
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ŒæŒ‰ RRF åˆ†æ•°é™åºæ’åˆ—
        """
        library_ids_to_search = library_ids if library_ids else [None]
        all_results: list[RetrievedChunk] = []
        
        # æŸ¥è¯¢æ‰©å±•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        expanded_query = await self.query_expander.expand_async(
            query,
            use_llm=self.settings.use_llm_expansion if hasattr(self, 'settings') and self.settings else False
        )
        if expanded_query != query:
            logger.debug(f"æŸ¥è¯¢æ‰©å±•: '{query}' â†’ '{expanded_query}'")
            query = expanded_query  # ä½¿ç”¨æ‰©å±•åçš„æŸ¥è¯¢
        
        # å¯¹æ¯ä¸ªåº“æ‰§è¡Œæ£€ç´¢
        for lib_id in library_ids_to_search:
            if use_hybrid:
                # æ··åˆæ£€ç´¢ï¼šå¹¶è¡Œæ‰§è¡Œå‘é‡æ£€ç´¢å’Œ BM25 æ£€ç´¢
                vector_results, bm25_results = await asyncio.gather(
                    self._vector_search(query, lib_id, top_k),
                    self._bm25_search(query, lib_id, top_k)
                )
                
                logger.debug(
                    f"Library {lib_id}: å‘é‡æ£€ç´¢ {len(vector_results)} æ¡, "
                    f"BM25 æ£€ç´¢ {len(bm25_results)} æ¡"
                )
                
                # RRF èåˆ
                if vector_results or bm25_results:
                    merged = _weighted_reciprocal_rank(vector_results, bm25_results)
                    all_results.extend(merged)
            else:
                # ä»…å‘é‡æ£€ç´¢ï¼ˆå›é€€åˆ°çˆ¶ç±»è¡Œä¸ºï¼‰
                vector_results = await self._vector_search(query, lib_id, top_k)
                all_results.extend(vector_results)
        
        if not all_results:
            logger.warning(f"æœªæ‰¾åˆ°ä»»ä½•ç»“æœ: query={query}, library_ids={library_ids}")
            return []
        
        # å»é‡å¹¶æ’åºï¼ˆæŒ‰ RRF åˆ†æ•°æˆ–ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
        # ä½¿ç”¨ document_id + text å‰50å­—ç¬¦ä½œä¸ºå»é‡é”®
        seen = set()
        unique_results = []
        for chunk in all_results:
            chunk_key = f"{chunk.document_id}:{chunk.text[:50]}"
            if chunk_key not in seen:
                seen.add(chunk_key)
                unique_results.append(chunk)
        
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        unique_results.sort(key=lambda c: c.score, reverse=True)
        
        # é‡æ’åºï¼šä½¿ç”¨ Cross-Encoder å¯¹ç­›é€‰åçš„å€™é€‰ç»“æœé‡æ–°æ’åº
        # è¿™æ ·å¯ä»¥å…ˆé€šè¿‡ RRF èåˆç­›é€‰å‡ºå€™é€‰ï¼Œå†ç”¨é‡æ’åºç²¾æ’
        if self.reranker.is_enabled() and len(unique_results) > 1:
            # è®¡ç®—é‡æ’åºå€™é€‰æ•°é‡
            # å¦‚æœé…ç½®äº†å›ºå®šæ•°é‡ï¼Œä½¿ç”¨é…ç½®å€¼ï¼›å¦åˆ™ä½¿ç”¨ top_k + 3ï¼ˆé€‚åº¦å‡å°‘ï¼‰
            if self.settings.rerank_candidate_count > 0:
                candidate_count = min(self.settings.rerank_candidate_count, len(unique_results))
            else:
                # é»˜è®¤ä½¿ç”¨ top_k + 3ï¼Œæ¯”åŸæ¥çš„ top_k * 2 æ›´å°‘ï¼Œæå‡é€Ÿåº¦
                candidate_count = min(top_k + 3, len(unique_results))
            
            rerank_candidates = unique_results[:candidate_count]
            # ä½¿ç”¨å¼‚æ­¥é‡æ’åºï¼ˆæ”¯æŒç¼“å­˜ï¼‰
            reranked = await self.reranker.rerank_async(query, rerank_candidates, top_k=top_k)
            logger.debug(
                f"é‡æ’åº: {len(rerank_candidates)} æ¡å€™é€‰ â†’ {len(reranked)} æ¡ç»“æœ"
            )
            return reranked
        
        return unique_results[:top_k]
    
    def invalidate_bm25_cache(self, library_id: UUID | None = None, chunk_ids: list[str] | None = None):
        """
        æ ‡è®°æŒ‡å®šåº“çš„ BM25 ç´¢å¼•éœ€è¦æ›´æ–°ï¼ˆå¢é‡æ›´æ–°æ ‡è®°ï¼‰ã€‚
        åœ¨æ–‡æ¡£æ·»åŠ /æ›´æ–°/åˆ é™¤åè°ƒç”¨æ­¤æ–¹æ³•ã€‚
        
        Args:
            library_id: æ–‡æ¡£åº“IDï¼ŒNone è¡¨ç¤ºæ ‡è®°æ‰€æœ‰åº“
            chunk_ids: å˜æ›´çš„ chunk ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºç»†ç²’åº¦è·Ÿè¸ªï¼‰
        """
        if library_id is None:
            # æ ‡è®°æ‰€æœ‰åº“ä¸ºè„æ•°æ®
            self._dirty_libraries.clear()
            # æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼ˆä¸‹æ¬¡æ£€ç´¢æ—¶ä¼šé‡å»ºï¼‰
            for lib_key in list(self._bm25_retrievers.keys()):
                self._dirty_libraries[lib_key] = set()
            logger.info("å·²æ ‡è®°æ‰€æœ‰ BM25 ç´¢å¼•éœ€è¦æ›´æ–°")
        else:
            lib_key = str(library_id)
            # æ ‡è®°ä¸ºè„æ•°æ®ï¼ˆä¸ç«‹å³åˆ é™¤ï¼Œå»¶è¿Ÿåˆ°ä¸‹æ¬¡æ£€ç´¢æ—¶é‡å»ºï¼‰
            if lib_key not in self._dirty_libraries:
                self._dirty_libraries[lib_key] = set()
            if chunk_ids:
                self._dirty_libraries[lib_key].update(chunk_ids)
            logger.debug(f"å·²æ ‡è®° library {library_id} çš„ BM25 ç´¢å¼•éœ€è¦æ›´æ–° (å˜æ›´ chunk æ•°: {len(chunk_ids) if chunk_ids else 'unknown'})")
    
    def force_rebuild_bm25_index(self, library_id: UUID | None = None):
        """
        å¼ºåˆ¶é‡å»ºæŒ‡å®šåº“çš„ BM25 ç´¢å¼•ï¼ˆç«‹å³é‡å»ºï¼Œä¸å»¶è¿Ÿï¼‰ã€‚
        é€‚ç”¨äºéœ€è¦ç«‹å³æ›´æ–°ç´¢å¼•çš„åœºæ™¯ã€‚
        
        Args:
            library_id: æ–‡æ¡£åº“IDï¼ŒNone è¡¨ç¤ºé‡å»ºæ‰€æœ‰åº“
        """
        if library_id is None:
            # é‡å»ºæ‰€æœ‰åº“
            for lib_key in list(self._bm25_retrievers.keys()):
                lib_id = UUID(lib_key) if lib_key else None
                self._reload_bm25_index(lib_id, force=True)
            logger.info("å·²å¼ºåˆ¶é‡å»ºæ‰€æœ‰ BM25 ç´¢å¼•")
        else:
            lib_key = str(library_id)
            # å¼ºåˆ¶é‡å»ºæŒ‡å®šåº“
            bm25_retriever = self._reload_bm25_index(library_id, force=True)
            self._bm25_retrievers[lib_key] = bm25_retriever
            # æ¸…é™¤è„æ•°æ®æ ‡è®°
            if lib_key in self._dirty_libraries:
                del self._dirty_libraries[lib_key]
            logger.info(f"å·²å¼ºåˆ¶é‡å»º library {library_id} çš„ BM25 ç´¢å¼•")

